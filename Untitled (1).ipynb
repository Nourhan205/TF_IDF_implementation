{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98f43e13-ea22-43e6-867f-fd9a202deccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf2 in e:\\programs\\aaaaaaanac_not\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de6e13e6-6fe3-4f26-890d-9e391979af39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import PyPDF2\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # تحويل إلى أحرف صغيرة\n",
    "    text = text.lower()\n",
    "    # إزالة علامات الترقيم\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # تقسيم النص إلى كلمات\n",
    "    tokens = word_tokenize(text)\n",
    "    # إزالة كلمات التوقف\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return tokens\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() + \" \"\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb0ad086-069c-41bd-b63c-0813f378f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_tf(document_tokens):\n",
    "    word_counts = Counter(document_tokens)\n",
    "    total_words = len(document_tokens)\n",
    "    tf = {word: count / total_words for word, count in word_counts.items()}\n",
    "    return tf\n",
    "\n",
    "def compute_idf(documents):\n",
    "    N = len(documents)\n",
    "    idf = {}\n",
    "    all_words = set(word for doc in documents for word in doc)\n",
    "    \n",
    "    for word in all_words:\n",
    "        df = sum(1 for doc in documents if word in doc)\n",
    "        idf[word] = math.log(N / df)\n",
    "    \n",
    "    return idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9340beb3-c889-47f5-ad85-6cc245522f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(documents):\n",
    "    tfidf_scores = []\n",
    "    idf = compute_idf(documents)\n",
    "    \n",
    "    for doc in documents:\n",
    "        tf = compute_tf(doc)\n",
    "        tfidf = {word: tf[word] * idf[word] for word in tf}\n",
    "        tfidf_scores.append(tfidf)\n",
    "    \n",
    "    return tfidf_scores\n",
    "\n",
    "def get_top_words(tfidf_scores, top_n=5):\n",
    "    top_words_per_doc = []\n",
    "    \n",
    "    for tfidf in tfidf_scores:\n",
    "        sorted_words = sorted(tfidf.items(), key=lambda x: x[1], reverse=True)\n",
    "        top_words_per_doc.append(sorted_words[:top_n])\n",
    "    \n",
    "    return top_words_per_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1d3e7e1-f7f6-4184-8e74-a9e3e4f35e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: reportlab in e:\\programs\\aaaaaaanac_not\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in e:\\programs\\aaaaaaanac_not\\lib\\site-packages (from reportlab) (10.4.0)\n",
      "Requirement already satisfied: chardet in e:\\programs\\aaaaaaanac_not\\lib\\site-packages (from reportlab) (4.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install reportlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af3f58e9-5a70-466c-9e90-c5180d2e5e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample PDFs created successfully!\n"
     ]
    }
   ],
   "source": [
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "def create_sample_pdf(file_name, text):\n",
    "    c = canvas.Canvas(file_name)\n",
    "    c.drawString(100, 750, text)\n",
    "    c.save()\n",
    "\n",
    "# إنشاء 4 ملفات PDF تجريبية\n",
    "texts = [\n",
    "    \"This is a sample document about artificial intelligence and deep learning.\",\n",
    "    \"Natural language processing is a key field in AI research and development.\",\n",
    "    \"Machine learning algorithms are improving every year with new techniques.\",\n",
    "    \"Supervised and unsupervised learning are two main types of machine learning.\"\n",
    "]\n",
    "\n",
    "for i in range(4):\n",
    "    create_sample_pdf(f\"doc{i+1}.pdf\", texts[i])\n",
    "\n",
    "print(\"Sample PDFs created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4977b0e9-768f-4760-a74b-b29321833557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1: [('sample', 0.23104906018664842), ('document', 0.23104906018664842), ('artificial', 0.23104906018664842), ('intelligence', 0.23104906018664842), ('deep', 0.23104906018664842)]\n",
      "Top words in document 2: [('natural', 0.17328679513998632), ('language', 0.17328679513998632), ('processing', 0.17328679513998632), ('key', 0.17328679513998632), ('field', 0.17328679513998632)]\n",
      "Top words in document 3: [('algorithms', 0.17328679513998632), ('improving', 0.17328679513998632), ('every', 0.17328679513998632), ('year', 0.17328679513998632), ('new', 0.17328679513998632)]\n",
      "Top words in document 4: [('supervised', 0.17328679513998632), ('unsupervised', 0.17328679513998632), ('two', 0.17328679513998632), ('main', 0.17328679513998632), ('types', 0.17328679513998632)]\n"
     ]
    }
   ],
   "source": [
    "pdf_files = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\", \"doc4.pdf\"]\n",
    "documents = [preprocess_text(extract_text_from_pdf(pdf)) for pdf in pdf_files]\n",
    "\n",
    "tfidf_scores = compute_tfidf(documents)\n",
    "top_words = get_top_words(tfidf_scores, top_n=5)\n",
    "\n",
    "for i, words in enumerate(top_words):\n",
    "    print(f\"Top words in document {i+1}: {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab588e2-4b42-4d11-8242-f8962b7d11b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
